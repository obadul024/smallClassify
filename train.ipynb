{"cells":[{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers.experimental.preprocessing import Normalization\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.python.keras import optimizers\n","from tensorflow.python.keras.applications.inception_v3 import preprocess_input"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-08-13 15:30:51.290487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2021-08-13 15:30:51.290552: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# WE ARE TRAINING ON FIVE CLASSES/ ACTORS \n","1. We Are training the model to recognise the five actors who played superman in the last 2 decades\n","2. The folder structure is as follows\n","            supermen/\n","                /cavill\n","                /welling\n","                /hoeclin\n","                /reeves\n","                /routh\n","\n","3. Each of these subdirectories acts as a label for the dataset\n","4. I took a single picture and augmented it to get a larger dataset\n","5. The augmentedModel.py file is given the name of the image, which has to be one of the subdirectory name, ending in a '.jpg' extension. This file resides in the smallClassify or the root directory\n","6. Then this script 'augmentedModel.py' is executed the folders are populated with augmented dataset\n","7. Do this for each of the actors. Change the filename, execute 'augmentedModel.py' script. Repeat.\n","8. Then See the JupyterNotebook for Comments"],"metadata":{}},{"cell_type":"markdown","source":["# Setting up the training generators\n","\n","The training and validation generators are created. THese supply images to the model one by one.\n","We split it into 80% training 20% validation set, see validtion_split. The classes are split into categorical mode to show that they are separate classes. Then the image size is set, for this InceptionV3 model, the INPUT Image Size has to be 299 x 299 x 3, i.e. three channels for color, also rescale the images to have pixel values from 0 to 255, this is important. Give it the training_dataset directory that we made using the augmented_Model.py script.\n","\n","\n"],"metadata":{}},{"cell_type":"code","execution_count":94,"source":["\n","train_data_dir = 'supermen'\n","batch_size = 16\n","img_height = 299\n","img_width = 299\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split=0.2) # set validation split\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training') # set as training data\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    train_data_dir, # same directory as training data\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation') # set as validation data"],"outputs":[{"output_type":"stream","name":"stdout","text":["Found 184 images belonging to 5 classes.\n","Found 45 images belonging to 5 classes.\n"]}],"metadata":{}},{"cell_type":"code","execution_count":72,"source":["# CREATE MODEL FROM INCEPTION_V3\n","# base_model loads the InceptionV3 model\n","# predictions add a new Dense layer with 5 neurons for our five classes\n","#then the model is create using base_model and predictions as its output\n","\n","base_model = InceptionV3(weights='imagenet', include_top=False)\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","predictions = Dense(5, activation='softmax')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":73,"source":["# LOAD SAVED MODEL\n","# to load a saved model uncomment this and run it \n","#model = keras.models.load_model('mymodel')\n","# model.summary()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Layers are set to not trainable\n","# we set original InceptionV3 model to freeze, so that out Dense layer has a chance to get initialised\n","\n","for layer in model.layers:\n","    layer.trainable = False\n","\n","\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","\n","# start fitting the model to get Dense layer initialsed\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch = train_generator.samples // batch_size,\n","    validation_data = validation_generator,\n","    validation_steps = validation_generator.samples // batch_size,\n","    epochs = 10)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":100,"source":["# Print layer names and then set the last layer few layers to unfreeze so we can train them to classify\n","\n","for i, layer in enumerate(model.layers):\n","    print(i, layer.name)\n","\n","for layer in model.layers[:249]:\n","    layer.trainable = False\n","\n","for layer in model.layers[249:]:\n","    layer.trainable = True\n","    print(layer.name)\n"," \n"],"outputs":[{"output_type":"stream","name":"stdout","text":["0 input_1\n","1 conv2d\n","2 batch_normalization\n","3 activation\n","4 conv2d_1\n","5 batch_normalization_1\n","6 activation_1\n","7 conv2d_2\n","8 batch_normalization_2\n","9 activation_2\n","10 max_pooling2d\n","11 conv2d_3\n","12 batch_normalization_3\n","13 activation_3\n","14 conv2d_4\n","15 batch_normalization_4\n","16 activation_4\n","17 max_pooling2d_1\n","18 conv2d_8\n","19 batch_normalization_8\n","20 activation_8\n","21 conv2d_6\n","22 conv2d_9\n","23 batch_normalization_6\n","24 batch_normalization_9\n","25 activation_6\n","26 activation_9\n","27 average_pooling2d\n","28 conv2d_5\n","29 conv2d_7\n","30 conv2d_10\n","31 conv2d_11\n","32 batch_normalization_5\n","33 batch_normalization_7\n","34 batch_normalization_10\n","35 batch_normalization_11\n","36 activation_5\n","37 activation_7\n","38 activation_10\n","39 activation_11\n","40 mixed0\n","41 conv2d_15\n","42 batch_normalization_15\n","43 activation_15\n","44 conv2d_13\n","45 conv2d_16\n","46 batch_normalization_13\n","47 batch_normalization_16\n","48 activation_13\n","49 activation_16\n","50 average_pooling2d_1\n","51 conv2d_12\n","52 conv2d_14\n","53 conv2d_17\n","54 conv2d_18\n","55 batch_normalization_12\n","56 batch_normalization_14\n","57 batch_normalization_17\n","58 batch_normalization_18\n","59 activation_12\n","60 activation_14\n","61 activation_17\n","62 activation_18\n","63 mixed1\n","64 conv2d_22\n","65 batch_normalization_22\n","66 activation_22\n","67 conv2d_20\n","68 conv2d_23\n","69 batch_normalization_20\n","70 batch_normalization_23\n","71 activation_20\n","72 activation_23\n","73 average_pooling2d_2\n","74 conv2d_19\n","75 conv2d_21\n","76 conv2d_24\n","77 conv2d_25\n","78 batch_normalization_19\n","79 batch_normalization_21\n","80 batch_normalization_24\n","81 batch_normalization_25\n","82 activation_19\n","83 activation_21\n","84 activation_24\n","85 activation_25\n","86 mixed2\n","87 conv2d_27\n","88 batch_normalization_27\n","89 activation_27\n","90 conv2d_28\n","91 batch_normalization_28\n","92 activation_28\n","93 conv2d_26\n","94 conv2d_29\n","95 batch_normalization_26\n","96 batch_normalization_29\n","97 activation_26\n","98 activation_29\n","99 max_pooling2d_2\n","100 mixed3\n","101 conv2d_34\n","102 batch_normalization_34\n","103 activation_34\n","104 conv2d_35\n","105 batch_normalization_35\n","106 activation_35\n","107 conv2d_31\n","108 conv2d_36\n","109 batch_normalization_31\n","110 batch_normalization_36\n","111 activation_31\n","112 activation_36\n","113 conv2d_32\n","114 conv2d_37\n","115 batch_normalization_32\n","116 batch_normalization_37\n","117 activation_32\n","118 activation_37\n","119 average_pooling2d_3\n","120 conv2d_30\n","121 conv2d_33\n","122 conv2d_38\n","123 conv2d_39\n","124 batch_normalization_30\n","125 batch_normalization_33\n","126 batch_normalization_38\n","127 batch_normalization_39\n","128 activation_30\n","129 activation_33\n","130 activation_38\n","131 activation_39\n","132 mixed4\n","133 conv2d_44\n","134 batch_normalization_44\n","135 activation_44\n","136 conv2d_45\n","137 batch_normalization_45\n","138 activation_45\n","139 conv2d_41\n","140 conv2d_46\n","141 batch_normalization_41\n","142 batch_normalization_46\n","143 activation_41\n","144 activation_46\n","145 conv2d_42\n","146 conv2d_47\n","147 batch_normalization_42\n","148 batch_normalization_47\n","149 activation_42\n","150 activation_47\n","151 average_pooling2d_4\n","152 conv2d_40\n","153 conv2d_43\n","154 conv2d_48\n","155 conv2d_49\n","156 batch_normalization_40\n","157 batch_normalization_43\n","158 batch_normalization_48\n","159 batch_normalization_49\n","160 activation_40\n","161 activation_43\n","162 activation_48\n","163 activation_49\n","164 mixed5\n","165 conv2d_54\n","166 batch_normalization_54\n","167 activation_54\n","168 conv2d_55\n","169 batch_normalization_55\n","170 activation_55\n","171 conv2d_51\n","172 conv2d_56\n","173 batch_normalization_51\n","174 batch_normalization_56\n","175 activation_51\n","176 activation_56\n","177 conv2d_52\n","178 conv2d_57\n","179 batch_normalization_52\n","180 batch_normalization_57\n","181 activation_52\n","182 activation_57\n","183 average_pooling2d_5\n","184 conv2d_50\n","185 conv2d_53\n","186 conv2d_58\n","187 conv2d_59\n","188 batch_normalization_50\n","189 batch_normalization_53\n","190 batch_normalization_58\n","191 batch_normalization_59\n","192 activation_50\n","193 activation_53\n","194 activation_58\n","195 activation_59\n","196 mixed6\n","197 conv2d_64\n","198 batch_normalization_64\n","199 activation_64\n","200 conv2d_65\n","201 batch_normalization_65\n","202 activation_65\n","203 conv2d_61\n","204 conv2d_66\n","205 batch_normalization_61\n","206 batch_normalization_66\n","207 activation_61\n","208 activation_66\n","209 conv2d_62\n","210 conv2d_67\n","211 batch_normalization_62\n","212 batch_normalization_67\n","213 activation_62\n","214 activation_67\n","215 average_pooling2d_6\n","216 conv2d_60\n","217 conv2d_63\n","218 conv2d_68\n","219 conv2d_69\n","220 batch_normalization_60\n","221 batch_normalization_63\n","222 batch_normalization_68\n","223 batch_normalization_69\n","224 activation_60\n","225 activation_63\n","226 activation_68\n","227 activation_69\n","228 mixed7\n","229 conv2d_72\n","230 batch_normalization_72\n","231 activation_72\n","232 conv2d_73\n","233 batch_normalization_73\n","234 activation_73\n","235 conv2d_70\n","236 conv2d_74\n","237 batch_normalization_70\n","238 batch_normalization_74\n","239 activation_70\n","240 activation_74\n","241 conv2d_71\n","242 conv2d_75\n","243 batch_normalization_71\n","244 batch_normalization_75\n","245 activation_71\n","246 activation_75\n","247 max_pooling2d_3\n","248 mixed8\n","249 conv2d_80\n","250 batch_normalization_80\n","251 activation_80\n","252 conv2d_77\n","253 conv2d_81\n","254 batch_normalization_77\n","255 batch_normalization_81\n","256 activation_77\n","257 activation_81\n","258 conv2d_78\n","259 conv2d_79\n","260 conv2d_82\n","261 conv2d_83\n","262 average_pooling2d_7\n","263 conv2d_76\n","264 batch_normalization_78\n","265 batch_normalization_79\n","266 batch_normalization_82\n","267 batch_normalization_83\n","268 conv2d_84\n","269 batch_normalization_76\n","270 activation_78\n","271 activation_79\n","272 activation_82\n","273 activation_83\n","274 batch_normalization_84\n","275 activation_76\n","276 mixed9_0\n","277 concatenate\n","278 activation_84\n","279 mixed9\n","280 conv2d_89\n","281 batch_normalization_89\n","282 activation_89\n","283 conv2d_86\n","284 conv2d_90\n","285 batch_normalization_86\n","286 batch_normalization_90\n","287 activation_86\n","288 activation_90\n","289 conv2d_87\n","290 conv2d_88\n","291 conv2d_91\n","292 conv2d_92\n","293 average_pooling2d_8\n","294 conv2d_85\n","295 batch_normalization_87\n","296 batch_normalization_88\n","297 batch_normalization_91\n","298 batch_normalization_92\n","299 conv2d_93\n","300 batch_normalization_85\n","301 activation_87\n","302 activation_88\n","303 activation_91\n","304 activation_92\n","305 batch_normalization_93\n","306 activation_85\n","307 mixed9_1\n","308 concatenate_1\n","309 activation_93\n","310 mixed10\n","311 global_average_pooling2d\n","312 dense\n","conv2d_80\n","batch_normalization_80\n","activation_80\n","conv2d_77\n","conv2d_81\n","batch_normalization_77\n","batch_normalization_81\n","activation_77\n","activation_81\n","conv2d_78\n","conv2d_79\n","conv2d_82\n","conv2d_83\n","average_pooling2d_7\n","conv2d_76\n","batch_normalization_78\n","batch_normalization_79\n","batch_normalization_82\n","batch_normalization_83\n","conv2d_84\n","batch_normalization_76\n","activation_78\n","activation_79\n","activation_82\n","activation_83\n","batch_normalization_84\n","activation_76\n","mixed9_0\n","concatenate\n","activation_84\n","mixed9\n","conv2d_89\n","batch_normalization_89\n","activation_89\n","conv2d_86\n","conv2d_90\n","batch_normalization_86\n","batch_normalization_90\n","activation_86\n","activation_90\n","conv2d_87\n","conv2d_88\n","conv2d_91\n","conv2d_92\n","average_pooling2d_8\n","conv2d_85\n","batch_normalization_87\n","batch_normalization_88\n","batch_normalization_91\n","batch_normalization_92\n","conv2d_93\n","batch_normalization_85\n","activation_87\n","activation_88\n","activation_91\n","activation_92\n","batch_normalization_93\n","activation_85\n","mixed9_1\n","concatenate_1\n","activation_93\n","mixed10\n","global_average_pooling2d\n","dense\n"]}],"metadata":{}},{"cell_type":"code","execution_count":96,"source":["# Compile our Model to use Stochastic Gradient Descent Algorithm with a very low Learning Rate so it can stop Underfitting\n","\n","from tensorflow.keras.optimizers import SGD\n","model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='categorical_crossentropy')\n","\n","model.fit_generator(\n","    \n","    train_generator,\n","    steps_per_epoch = train_generator.samples // batch_size,\n","    validation_data = validation_generator,\n","    validation_steps = validation_generator.samples // batch_size,\n","    epochs = 30)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","11/11 [==============================] - 20s 2s/step - loss: 1.2015 - val_loss: 1.1771\n","Epoch 2/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.9015 - val_loss: 0.9425\n","Epoch 3/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.7145 - val_loss: 0.6704\n","Epoch 4/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.5891 - val_loss: 0.5146\n","Epoch 5/30\n","11/11 [==============================] - 16s 2s/step - loss: 0.5032 - val_loss: 0.4410\n","Epoch 6/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.4243 - val_loss: 0.4082\n","Epoch 7/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.3590 - val_loss: 0.3564\n","Epoch 8/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.3159 - val_loss: 0.2524\n","Epoch 9/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.2989 - val_loss: 0.2437\n","Epoch 10/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.2873 - val_loss: 0.2483\n","Epoch 11/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.2541 - val_loss: 0.1825\n","Epoch 12/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.2159 - val_loss: 0.1972\n","Epoch 13/30\n","11/11 [==============================] - 18s 2s/step - loss: 0.2364 - val_loss: 0.1829\n","Epoch 14/30\n","11/11 [==============================] - 18s 2s/step - loss: 0.1869 - val_loss: 0.1441\n","Epoch 15/30\n","11/11 [==============================] - 18s 2s/step - loss: 0.1845 - val_loss: 0.1306\n","Epoch 16/30\n","11/11 [==============================] - 18s 2s/step - loss: 0.1511 - val_loss: 0.1433\n","Epoch 17/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1529 - val_loss: 0.1042\n","Epoch 18/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1349 - val_loss: 0.1224\n","Epoch 19/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1350 - val_loss: 0.1041\n","Epoch 20/30\n","11/11 [==============================] - 16s 2s/step - loss: 0.1348 - val_loss: 0.0871\n","Epoch 21/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1346 - val_loss: 0.0881\n","Epoch 22/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1083 - val_loss: 0.0941\n","Epoch 23/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1068 - val_loss: 0.0848\n","Epoch 24/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1063 - val_loss: 0.0796\n","Epoch 25/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1051 - val_loss: 0.0598\n","Epoch 26/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.0951 - val_loss: 0.0649\n","Epoch 27/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1080 - val_loss: 0.0591\n","Epoch 28/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.1003 - val_loss: 0.0545\n","Epoch 29/30\n","11/11 [==============================] - 16s 1s/step - loss: 0.0890 - val_loss: 0.0615\n","Epoch 30/30\n","11/11 [==============================] - 17s 2s/step - loss: 0.0793 - val_loss: 0.0506\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f76ddec5490>"]},"metadata":{},"execution_count":96}],"metadata":{}},{"cell_type":"code","execution_count":79,"source":["#Save the model as retrained_model\n","model.save('retrained_model')"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-08-13 17:07:34.375263: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: retrained_model/assets\n"]}],"metadata":{}},{"cell_type":"markdown","source":["# IGNORE THIS CELL"],"metadata":{}},{"cell_type":"code","execution_count":101,"source":["# test_data_dir = 'test'\n","\n","\n","# test_datagen = ImageDataGenerator(\n","#     rescale=1./255,\n","#     ) # set validation split\n","\n","\n","\n","\n","# test_generator = test_datagen.flow_from_directory(\n","#     test_data_dir,\n","#     target_size=(img_height, img_width),\n","#     batch_size=1,\n","#     class_mode='categorical',\n","#     subset='validation') # set as test data\n","# unseen_img = '.jpeg'\n","# img = image.load_img(unseen_img, target_size=(img_height, img_width))\n","# x = image.img_to_array(img)\n","# x = np.expand_dims(x, axis=0)\n","# x = preprocess_input(x)\n","# y_prob = model.predict(x)\n","# list(train_generator.class_indices)[np.argmax(y_prob)]"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Test the model on test set\n","\n","input images into the test folder as is done in supermen directory\n","and run this block\n","It outputs what the Predicted Result is Versus the Actual Name\n","\n","N.B I need to code it to give accrucy values, for now the accuracy is around 30%"],"metadata":{}},{"cell_type":"code","execution_count":92,"source":["\n","import glob\n","\n","for filepath in glob.iglob('./test/*/*'):\n","    \n","    img = image.load_img(filepath, target_size=(img_height, img_width))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    y_prob = model.predict(x)\n","    print(list(train_generator.class_indices)[np.argmax(y_prob)], filepath[14:])\n","    # print(y_prob, np.argmax(y_prob)+1, filepath[14:])\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.15501486 0.21800774 0.10289738 0.16331369 0.3607663 ]] 5 /tyler3.jpeg\n","[[0.12466992 0.34644377 0.16873373 0.17250645 0.18764617]] 2 /tyler.jpeg\n","[[0.01094298 0.9544403  0.01444672 0.00981366 0.01035633]] 2 /hoeclin.jpg\n","[[0.16317339 0.18183959 0.21899135 0.22014625 0.21584934]] 4 /tyler2.jpeg\n","[[0.25186446 0.30698174 0.09546199 0.2207594  0.1249324 ]] 2 /welling224.jpeg\n","[[0.12573938 0.26510295 0.26250795 0.25101313 0.09563664]] 2 /welling3444.jpeg\n","[[0.18175295 0.3003572  0.09247416 0.2601252  0.16529052]] 2 /welling (1).jpeg\n","[[0.31992063 0.23612274 0.16281693 0.15921114 0.12192853]] 1 /welling.jpeg\n","[[0.09524105 0.2261723  0.2834292  0.19061413 0.2045433 ]] 3 /welling211113.jpeg\n","[[0.27116245 0.14984548 0.29829383 0.16736977 0.11332853]] 3 outh.jpeg\n","[[0.34925246 0.239457   0.13798626 0.15341815 0.11988615]] 1 outh2.jpeg\n","[[0.28725067 0.28023827 0.17587826 0.16196305 0.09466974]] 1 cavill23556.jpeg\n","[[0.1849157  0.18616256 0.10924022 0.13056192 0.38911954]] 5 cavill2.jpg\n","[[0.15344335 0.10673031 0.15290466 0.26424694 0.32267472]] 5 cavill234.jpeg\n","[[0.15781069 0.18734583 0.15182629 0.33172762 0.17128961]] 4 cavill23662636.jpeg\n","[[0.31174004 0.11028787 0.21823627 0.28708714 0.07264867]] 1 cavil34666.jpeg\n","[[0.27392957 0.17824994 0.24274898 0.214479   0.09059253]] 1 cavill362662.jpeg\n"]}],"metadata":{}},{"cell_type":"code","execution_count":88,"source":["train_generator.class_indices"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'cavill': 0, 'hoeclin': 1, 'reeves': 2, 'routh': 3, 'welling': 4}"]},"metadata":{},"execution_count":88}],"metadata":{}},{"cell_type":"code","execution_count":30,"source":["model.save('mymodel')"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-08-13 06:41:44.552922: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: mymodel/assets\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('small': venv)"},"interpreter":{"hash":"9d445c912014069023a257f2dbf397f4fe171727110377efb3710a5aa6ffde68"}}}